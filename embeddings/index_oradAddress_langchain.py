"""
ìµœì¢… 3ì¸ë±ìŠ¤ ê³ ì† ë³‘ë ¬ RAG ì‹œìŠ¤í…œ
Road + Jibun + Building (ì§€ì—­í¬í•¨, aliasesì œê±°)
"""

import os
import json
import time
import re
import psycopg2
import psycopg2.extras
from datetime import datetime
from typing import List, Dict, Any, Generator, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor
import queue
import threading

# LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import OpenSearchVectorSearch
from langchain.schema import Document

# OpenSearch ì—°ê²°
import requests
from requests.auth import HTTPBasicAuth

# ì„¤ì • (config íŒŒì¼ì—ì„œ ë¡œë“œ)
from config import (
    OPENSEARCH_CONFIG4, 
    POSTGRESQL_CONFIG,
    SEARCH_CONFIG,
    INDEX_CONFIG
)

class FinalThreeIndexRAG:
    """ìµœì¢… 3ì¸ë±ìŠ¤ RAG ì‹œìŠ¤í…œ - Road, Jibun, Building"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """ìµœì¢… 3ì¸ë±ìŠ¤ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        # ì„¤ì • ë¡œë“œ (ê¸°ì¡´ config íŒŒì¼ ì°¸ì¡°)
        opensearch_config = config.get('opensearch', OPENSEARCH_CONFIG4) if config else OPENSEARCH_CONFIG4
        self.postgresql_config = config.get('postgresql', POSTGRESQL_CONFIG) if config else POSTGRESQL_CONFIG
        
        # OpenSearch ì—°ê²° ì„¤ì •
        self.opensearch_url = opensearch_config.get('url')
        self.base_index_name = opensearch_config.get('index_name', 'ko-address')
        self.username = opensearch_config.get('username')
        self.password = opensearch_config.get('password')
        self.verify_ssl = opensearch_config.get('verify_ssl', False)
        self.auth = HTTPBasicAuth(self.username, self.password)
        self.headers = {'Content-Type': 'application/json'}
        
        # 3ê°œ ì¸ë±ìŠ¤ ì„¤ì •
        self.indexes = {
            'road': f"{self.base_index_name}-road",         # ë„ë¡œëª…ì£¼ì†Œ ì „ìš©
            'jibun': f"{self.base_index_name}-jibun",       # ì§€ë²ˆì£¼ì†Œ ì „ìš©
            'building': f"{self.base_index_name}-building"   # ë¹Œë”©ëª…+ì§€ì—­ ì „ìš©
        }
        
        # í•œêµ­ì–´ SRoBERTa ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print("ğŸ¤– í•œêµ­ì–´ SRoBERTa ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-multitask',
            model_kwargs={
                'device': 'cpu',
                'trust_remote_code': True
            },
            encode_kwargs={
                'normalize_embeddings': True,
                'batch_size': 16
            }
        )
        print("âœ… í•œêµ­ì–´ SRoBERTa ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ë²¡í„° ìŠ¤í† ì–´ë“¤
        self.vector_stores = {}
        
        # PostgreSQL ì—°ê²°
        self.pg_connection = None
        
        # í†µê³„
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'start_time': None
        }

    def connect_postgresql(self) -> bool:
        """PostgreSQL ì—°ê²°"""
        try:
            self.pg_connection = psycopg2.connect(
                host=self.postgresql_config.get('host'),
                port=self.postgresql_config.get('port'),
                database=self.postgresql_config.get('database'),
                user=self.postgresql_config.get('user'),
                password=self.postgresql_config.get('password'),
                sslmode=self.postgresql_config.get('sslmode', 'disable')
            )
            self.pg_connection.autocommit = False
            print("âœ… PostgreSQL ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            print(f"âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def check_opensearch_connection(self) -> bool:
        """OpenSearch ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(
                self.opensearch_url,
                auth=self.auth,
                headers=self.headers,
                verify=self.verify_ssl,
                timeout=120
            )
            print(f"âœ… OpenSearch ì—°ê²° í™•ì¸: {response.status_code}")
            return response.status_code < 400
        except Exception as e:
            print(f"âŒ OpenSearch ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def create_three_indexes(self) -> bool:
        """3ê°œ ì¸ë±ìŠ¤ ìƒì„± - Road, Jibun, Building"""
        
        print("ğŸ”§ 3ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘ (Road + Jibun + Building)")
        
        # OpenSearch ì—°ê²° ì¬í™•ì¸
        if not self.check_opensearch_connection():
            print("âŒ OpenSearch ì—°ê²° ì‹¤íŒ¨ - ì¸ë±ìŠ¤ ìƒì„± ì¤‘ë‹¨")
            return False
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ë“¤ ì‚­ì œ
        for index_type, index_name in self.indexes.items():
            self._delete_index_if_exists(index_name)
        
        # ê³µí†µ ì¸ë±ìŠ¤ ì„¤ì •
        base_config = {
            "settings": {
                "index": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "refresh_interval": "30s",
                    "knn": True,
                    "max_ngram_diff": 5,
                    "analysis": {
                        "analyzer": {
                            "korean_analyzer": {
                                "type": "custom",
                                "tokenizer": "nori_tokenizer",
                                "filter": ["lowercase", "nori_part_of_speech"]
                            },
                            "address_analyzer": {
                                "type": "custom",
                                "tokenizer": "keyword",
                                "filter": ["lowercase", "trim"]
                            },
                            "address_ngram_analyzer": {
                                "type": "custom",
                                "tokenizer": "standard",
                                "filter": ["lowercase", "address_ngram_filter"]
                            },
                            "address_search_analyzer": {
                                "type": "custom",
                                "tokenizer": "standard",
                                "filter": ["lowercase"]
                            }
                        },
                        "filter": {
                            "address_ngram_filter": {
                                "type": "ngram",
                                "min_gram": 2,
                                "max_gram": 4,
                                "token_chars": ["letter", "digit"]
                            }
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "content": {
                        "type": "text",
                        "analyzer": "korean_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "address": {"type": "text", "analyzer": "address_analyzer"}
                        }
                    },
                    "address_field": {
                        "type": "text",
                        "analyzer": "address_analyzer",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "ngram": {
                                "type": "text",
                                "analyzer": "address_ngram_analyzer",
                                "search_analyzer": "address_search_analyzer"
                            },
                            "partial": {
                                "type": "text",
                                "analyzer": "korean_analyzer"
                            }
                        }
                    },
                    "vector_field": {
                        "type": "knn_vector",
                        "dimension": 768,
                        "method": {
                            "name": "hnsw",
                            "space_type": "cosinesimil",
                            "engine": "nmslib",
                            "parameters": {
                                "ef_construction": 128,
                                "m": 16
                            }
                        }
                    },
                    "postal_code": {"type": "keyword"},
                    "region": {"type": "keyword"},
                    "city": {"type": "keyword"},
                    "district": {"type": "keyword"},
                    "building_name": {"type": "keyword"},
                    "search_type": {"type": "keyword"},
                    "created_at": {"type": "date"}
                }
            }
        }
        
        # ê° ì¸ë±ìŠ¤ ìƒì„±
        created_indexes = []
        
        for index_type, index_name in self.indexes.items():
            try:
                print(f"ğŸ”¨ {index_type} ì¸ë±ìŠ¤ ìƒì„± ì¤‘: {index_name}")
                
                response = requests.put(
                    f"{self.opensearch_url}/{index_name}",
                    headers=self.headers,
                    data=json.dumps(base_config),
                    auth=self.auth,
                    verify=self.verify_ssl,
                    timeout=120
                )
                
                if response.status_code in [200, 201]:
                    print(f"  âœ… {index_type} ì¸ë±ìŠ¤ ìƒì„± ì„±ê³µ: {index_name}")
                    created_indexes.append(index_name)
                    time.sleep(2)
                else:
                    print(f"  âŒ {index_type} ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {response.status_code}")
                    print(f"  ğŸ“„ ì‘ë‹µ: {response.text[:500]}")
                    
                    # ì´ë¯¸ ìƒì„±ëœ ì¸ë±ìŠ¤ë“¤ ì •ë¦¬
                    for created_index in created_indexes:
                        self._delete_index_if_exists(created_index)
                    return False
                    
            except Exception as e:
                print(f"  âŒ {index_type} ì¸ë±ìŠ¤ ìƒì„± ì˜¤ë¥˜: {e}")
                for created_index in created_indexes:
                    self._delete_index_if_exists(created_index)
                return False
        
        # ì¸ë±ìŠ¤ ì•ˆì •í™” ëŒ€ê¸°
        print("â³ ì¸ë±ìŠ¤ ì•ˆì •í™” ëŒ€ê¸° ì¤‘...")
        time.sleep(10)
        
        # ìµœì¢… í™•ì¸
        all_created = True
        for index_type, index_name in self.indexes.items():
            try:
                response = requests.head(
                    f"{self.opensearch_url}/{index_name}",
                    auth=self.auth,
                    verify=self.verify_ssl,
                    timeout=120
                )
                
                if response.status_code == 200:
                    print(f"  âœ… {index_type} ì¸ë±ìŠ¤ ìµœì¢… í™•ì¸ ì™„ë£Œ: {index_name}")
                else:
                    print(f"  âŒ {index_type} ì¸ë±ìŠ¤ ìµœì¢… í™•ì¸ ì‹¤íŒ¨: {response.status_code}")
                    all_created = False
                    
            except Exception as e:
                print(f"  âŒ {index_type} ì¸ë±ìŠ¤ í™•ì¸ ì˜¤ë¥˜: {e}")
                all_created = False
        
        if all_created:
            print("ğŸ‰ ëª¨ë“  3ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
            print(f"ğŸ“‚ ìƒì„±ëœ ì¸ë±ìŠ¤: {len(self.indexes)}ê°œ")
            for index_type, index_name in self.indexes.items():
                print(f"  â€¢ {index_type.upper()}: {index_name}")
            return True
        else:
            print("âŒ ì¼ë¶€ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            return False

    def _delete_index_if_exists(self, index_name: str):
        """ì¸ë±ìŠ¤ ì‚­ì œ"""
        try:
            response = requests.head(
                f"{self.opensearch_url}/{index_name}",
                auth=self.auth,
                verify=self.verify_ssl
            )
            
            if response.status_code == 200:
                print(f"ê¸°ì¡´ ì¸ë±ìŠ¤ {index_name} ì‚­ì œ ì¤‘...")
                delete_response = requests.delete(
                    f"{self.opensearch_url}/{index_name}",
                    auth=self.auth,
                    verify=self.verify_ssl,
                    timeout=120
                )
                
                if delete_response.status_code in [200, 404]:
                    print(f"ê¸°ì¡´ ì¸ë±ìŠ¤ {index_name} ì‚­ì œ ì™„ë£Œ")
                    time.sleep(2)
                    
        except Exception as e:
            print(f"ì¸ë±ìŠ¤ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

    def extract_address_components(self, address: str) -> Dict[str, str]:
        """ì£¼ì†Œì—ì„œ í•µì‹¬ êµ¬ì„±ìš”ì†Œ ì¶”ì¶œ"""
        components = {
            "region": "",      # ì‹œ/ë„
            "city": "",        # ì‹œ/êµ°/êµ¬  
            "district": "",    # ë™/ì/ë©´
            "road": "",        # ë„ë¡œëª…
            "dong": "",        # ë™ëª… (ì§€ë²ˆìš©)
            "building": ""     # ê±´ë¬¼ëª…
        }
        
        if not address:
            return components
        
        # ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì¶”ì¶œ (ë™ëª…, ê±´ë¬¼ëª…)
        parenthesis_match = re.search(r'\(([^)]+)\)', address)
        if parenthesis_match:
            parenthesis_content = parenthesis_match.group(1)
            if ',' in parenthesis_content:
                parts = parenthesis_content.split(',')
                components["dong"] = parts[0].strip()
                components["building"] = parts[1].strip() if len(parts) > 1 else ""
            else:
                if 'ë™' in parenthesis_content or 'ì' in parenthesis_content or 'ë©´' in parenthesis_content:
                    components["dong"] = parenthesis_content.strip()
                else:
                    components["building"] = parenthesis_content.strip()
        
        # ê´„í˜¸ ì œê±°í•˜ê³  ê¸°ë³¸ ì£¼ì†Œ íŒŒì‹±
        clean_address = re.sub(r'\([^)]*\)', '', address).strip()
        parts = clean_address.split()
        
        if len(parts) >= 1:
            components["region"] = parts[0]  # ì„œìš¸íŠ¹ë³„ì‹œ, ê²½ê¸°ë„ ë“±
        if len(parts) >= 2:
            components["city"] = parts[1]    # ê°•ë‚¨êµ¬, ì„±ë‚¨ì‹œ ë“±
        if len(parts) >= 3:
            # ë„ë¡œëª… ì°¾ê¸°
            for i, part in enumerate(parts[2:], 2):
                if 'ë¡œ' in part or 'ê¸¸' in part or 'ëŒ€ë¡œ' in part:
                    components["road"] = part
                    break
        
        return components

    def get_sido_short(self, sido: str) -> str:
        """ì‹œë„ëª… ë‹¨ì¶•í˜• ë³€í™˜"""
        sido_mapping = {
            'ì„œìš¸íŠ¹ë³„ì‹œ': 'ì„œìš¸',
            'ë¶€ì‚°ê´‘ì—­ì‹œ': 'ë¶€ì‚°', 
            'ëŒ€êµ¬ê´‘ì—­ì‹œ': 'ëŒ€êµ¬',
            'ì¸ì²œê´‘ì—­ì‹œ': 'ì¸ì²œ',
            'ê´‘ì£¼ê´‘ì—­ì‹œ': 'ê´‘ì£¼',
            'ëŒ€ì „ê´‘ì—­ì‹œ': 'ëŒ€ì „',
            'ìš¸ì‚°ê´‘ì—­ì‹œ': 'ìš¸ì‚°',
            'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ': 'ì„¸ì¢…',
            'ê²½ê¸°ë„': 'ê²½ê¸°',
            'ê°•ì›ë„': 'ê°•ì›',
            'ì¶©ì²­ë¶ë„': 'ì¶©ë¶',
            'ì¶©ì²­ë‚¨ë„': 'ì¶©ë‚¨',
            'ì „ë¼ë¶ë„': 'ì „ë¶',
            'ì „ë¼ë‚¨ë„': 'ì „ë‚¨',
            'ê²½ìƒë¶ë„': 'ê²½ë¶',
            'ê²½ìƒë‚¨ë„': 'ê²½ë‚¨',
            'ì œì£¼íŠ¹ë³„ìì¹˜ë„': 'ì œì£¼'
        }
        return sido_mapping.get(sido, sido)

    def get_sigungu_short(self, sigungu: str) -> str:
        """ì‹œêµ°êµ¬ëª… ë‹¨ì¶•í˜• ë³€í™˜"""
        if sigungu.endswith('êµ¬'):
            return sigungu[:-1]  # ê°•ë‚¨êµ¬ â†’ ê°•ë‚¨
        elif sigungu.endswith('ì‹œ'):
            return sigungu[:-1]  # ìˆ˜ì›ì‹œ â†’ ìˆ˜ì›
        elif sigungu.endswith('êµ°'):
            return sigungu[:-1]  # ì–‘í‰êµ° â†’ ì–‘í‰
        return sigungu

    def get_dong_short(self, dong: str) -> str:
        """ë™ëª… ë‹¨ì¶•í˜• ë³€í™˜"""
        if dong.endswith('ë™'):
            return dong[:-1]  # ì—­ì‚¼ë™ â†’ ì—­ì‚¼
        elif dong.endswith('ì'):
            return dong[:-1]  # ì‹ ë„ì â†’ ì‹ ë„
        elif dong.endswith('ë©´'):
            return dong[:-1]  # ì™¸ë™ë©´ â†’ ì™¸ë™
        return dong

    def extract_building_number(self, address: str) -> str:
        """ì£¼ì†Œì—ì„œ ê±´ë¬¼ë²ˆí˜¸ ì •í™•íˆ ì¶”ì¶œ"""
        if not address:
            return ""
        
        # ë„ë¡œëª… ë’¤ì˜ ì²« ë²ˆì§¸ ìˆ«ì íŒ¨í„´
        road_number_pattern = r'(?:ë¡œ|ê¸¸|ëŒ€ë¡œ)\s+(\d+(?:-\d+)?)'
        match = re.search(road_number_pattern, address)
        
        if match:
            return match.group(1)
        
        # ì¼ë°˜ì ì¸ ìˆ«ì íŒ¨í„´ (ë§ˆì§€ë§‰ ìˆ«ì)
        number_patterns = re.findall(r'\d+(?:-\d+)?', address)
        if number_patterns:
            return number_patterns[-1]
        
        return ""

    def create_road_content(self, road_address: str) -> str:
        """ë„ë¡œëª…ì£¼ì†Œ ì½˜í…ì¸  ìƒì„± (ë¹Œë”©ëª… ì œì™¸)"""
        if not road_address:
            return ""
        
        # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ë™ëª…, ë¹Œë”©ëª… ëª¨ë‘ ì œê±°)
        clean_address = re.sub(r'\([^)]*\)', '', road_address).strip()
        
        # ì£¼ì†Œ êµ¬ì„±ìš”ì†Œ ì¶”ì¶œ
        components = self.extract_address_components(clean_address)
        
        # ìˆœìˆ˜ ì£¼ì†Œ ì •ë³´ë§Œ ì¡°í•©
        road_parts = []
        
        if components.get('region'):
            road_parts.append(components['region'])
        
        if components.get('city'):
            road_parts.append(components['city'])
        
        if components.get('road'):
            road_parts.append(components['road'])
        
        # ê±´ë¬¼ë²ˆí˜¸
        building_number = self.extract_building_number(clean_address)
        if building_number:
            road_parts.append(building_number)
        
        return " ".join(road_parts)

    def create_jibun_content(self, jibun_address: str) -> str:
        """ì§€ë²ˆì£¼ì†Œ ì½˜í…ì¸  ìƒì„±"""
        if not jibun_address:
            return ""
        
        # ê´„í˜¸ ì œê±°í•˜ê³  ìˆœìˆ˜ ì§€ë²ˆì£¼ì†Œë§Œ
        clean_address = re.sub(r'\([^)]*\)', '', jibun_address).strip()
        return clean_address

    def create_building_content(self, building_name: str, road_address: str) -> str:
        """ë¹Œë”© ì½˜í…ì¸  ìƒì„± (ì§€ì—­ í¬í•¨, aliases ì œê±°)"""
        if not building_name:
            return ""
        
        content_parts = [building_name]
        
        # ì§€ì—­ ì •ë³´ ì¶”ì¶œ
        components = self.extract_address_components(road_address)
        
        # ì‹œë„ + ë¹Œë”©ëª…
        sido_short = self.get_sido_short(components.get('region', ''))
        if sido_short:
            content_parts.append(f"{sido_short} {building_name}")
        
        # ì‹œêµ°êµ¬ + ë¹Œë”©ëª…
        sigungu = components.get('city', '')
        if sigungu:
            content_parts.append(f"{sigungu} {building_name}")
            
            sigungu_short = self.get_sigungu_short(sigungu)
            if sigungu_short != sigungu:
                content_parts.append(f"{sigungu_short} {building_name}")
        
        # ë™ + ë¹Œë”©ëª…
        dong = components.get('dong', '')
        if dong:
            content_parts.append(f"{dong} {building_name}")
            
            dong_short = self.get_dong_short(dong)
            if dong_short != dong:
                content_parts.append(f"{dong_short} {building_name}")
        
        return " ".join(content_parts)

    def get_address_count(self) -> int:
        """ì£¼ì†Œ ë°ì´í„° ì´ ê°œìˆ˜ ì¡°íšŒ"""
        try:
            cursor = self.pg_connection.cursor()
            
            count_query = """
            SELECT COUNT(*) 
            FROM address.building_info
            WHERE road_name IS NOT NULL 
              AND building_main_number IS NOT NULL
            """
            
            cursor.execute(count_query)
            total_count = cursor.fetchone()[0]
            cursor.close()
            self.pg_connection.commit()
            
            print(f"ğŸ“Š ì´ ì²˜ë¦¬ ëŒ€ìƒ: {total_count:,}ê°œ ì£¼ì†Œ")
            return total_count
            
        except Exception as e:
            print(f"âŒ ì¹´ìš´íŠ¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            if self.pg_connection:
                self.pg_connection.rollback()
            return 0

    def stream_address_documents(self, batch_size: int = 1000) -> Generator[Dict[str, List[Document]], None, None]:
        """PostgreSQLì—ì„œ ì£¼ì†Œ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê°€ì ¸ì™€ 3ì¸ë±ìŠ¤ Document ìƒì„±"""
        
        try:
            cursor = self.pg_connection.cursor(name='address_cursor')
            
            query = """
            SELECT DISTINCT 
                road_address, 
                jibun_address, 
                base_zip_code,
                building_name
            FROM (
                SELECT 
                    CASE 
                        WHEN COALESCE(building_sub_number, 0) = 0 THEN 
                            COALESCE(sido_name, '') || ' ' || 
                            COALESCE(road_name, '') || ' ' || 
                            COALESCE(building_main_number::TEXT, '') ||
                            CASE 
                                WHEN COALESCE(eupmyeondong_name, '') != '' THEN 
                                    ' (' || eupmyeondong_name || 
                                        CASE 
                                            WHEN sigungu_building_name IS NOT NULL THEN ', ' || sigungu_building_name 
                                            ELSE '' 
                                        END || ')'
                                ELSE ''
                            END
                        ELSE
                            COALESCE(sido_name, '') || ' ' || 
                            COALESCE(road_name, '') || ' ' || 
                            COALESCE(building_main_number::TEXT, '') || '-' || 
                            COALESCE(building_sub_number::TEXT, '') ||
                            CASE 
                                WHEN COALESCE(eupmyeondong_name, '') != '' THEN 
                                    ' (' || eupmyeondong_name || 
                                        CASE 
                                            WHEN sigungu_building_name IS NOT NULL THEN ', ' || sigungu_building_name 
                                            ELSE '' 
                                        END || ')'
                                ELSE ''
                            END
                    END AS road_address,   
                    CASE 
                        WHEN COALESCE(lot_sub_number, 0) = 0 THEN 
                            COALESCE(sido_name, '') || ' ' || 
                            COALESCE(eupmyeondong_name, '') || ' ' || 
                            COALESCE(legal_ri_name, '') || ' ' || 
                            COALESCE(lot_main_number::TEXT, '')
                        ELSE
                            COALESCE(sido_name, '') || ' ' || 
                            COALESCE(eupmyeondong_name, '') || ' ' || 
                            COALESCE(legal_ri_name, '') || ' ' || 
                            COALESCE(lot_main_number::TEXT, '') || '-' || 
                            COALESCE(lot_sub_number::TEXT, '')
                    END AS jibun_address,
                    base_zip_code,
                    sigungu_building_name as building_name
                FROM address.building_info
                WHERE road_name IS NOT NULL 
                  AND building_main_number IS NOT NULL
            ) addr_data
            ORDER BY road_address
            """
            
            print("ğŸ” PostgreSQL ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
            cursor.execute(query)
            
            batch_count = 0
            while True:
                rows = cursor.fetchmany(batch_size)
                if not rows:
                    break
                
                batch_count += 1
                print(f"ğŸ“¦ ë°°ì¹˜ {batch_count} ë¡œë“œ: {len(rows)}ê°œ ë ˆì½”ë“œ")
                
                # 3ì¸ë±ìŠ¤ Document ìƒì„±
                document_sets = {"road": [], "jibun": [], "building": []}
                
                for row in rows:
                    road_address = row[0] if row[0] else ""
                    jibun_address = row[1] if row[1] else ""
                    postal_code = row[2] if row[2] else ""
                    building_name = row[3] if row[3] else ""
                    
                    # ì£¼ì†Œ êµ¬ì„±ìš”ì†Œ ì¶”ì¶œ
                    components = self.extract_address_components(road_address)
                    
                    # ê³µí†µ ë©”íƒ€ë°ì´í„°
                    base_metadata = {
                        "road_address": road_address.strip(),
                        "jibun_address": jibun_address.strip(),
                        "postal_code": postal_code.strip(),
                        "region": components.get('region', ''),
                        "city": components.get('city', ''),
                        "district": components.get('dong', ''),
                        "building_name": building_name.strip(),
                        "created_at": datetime.now().isoformat()
                    }
                    
                    # Road Document (ë„ë¡œëª…ì£¼ì†Œ, ë¹Œë”©ëª… ì œì™¸)
                    road_content = self.create_road_content(road_address)
                    if road_content.strip():
                        road_doc = Document(
                            page_content=road_content,
                            metadata={**base_metadata, "search_type": "road"}
                        )
                        document_sets["road"].append(road_doc)
                    
                    # Jibun Document (ì§€ë²ˆì£¼ì†Œ)
                    if jibun_address and jibun_address != road_address:
                        jibun_content = self.create_jibun_content(jibun_address)
                        if jibun_content.strip():
                            jibun_doc = Document(
                                page_content=jibun_content,
                                metadata={**base_metadata, "search_type": "jibun"}
                            )
                            document_sets["jibun"].append(jibun_doc)
                    
                    # Building Document (ë¹Œë”©ëª…+ì§€ì—­, aliases ì œê±°)
                    if building_name:
                        building_content = self.create_building_content(building_name, road_address)
                        if building_content.strip():
                            building_doc = Document(
                                page_content=building_content,
                                metadata={**base_metadata, "search_type": "building"}
                            )
                            document_sets["building"].append(building_doc)
                
                yield document_sets
            
            cursor.close()
            self.pg_connection.commit()
            print(f"âœ… ì´ {batch_count}ê°œ ë°°ì¹˜ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì£¼ì†Œ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            if self.pg_connection:
                self.pg_connection.rollback()

    def initialize_vector_stores(self):
        """ë²¡í„° ìŠ¤í† ì–´ë“¤ ì´ˆê¸°í™”"""
        try:
            for index_type, index_name in self.indexes.items():
                self.vector_stores[index_type] = OpenSearchVectorSearch(
                    index_name=index_name,
                    embedding_function=self.embeddings,
                    opensearch_url=self.opensearch_url,
                    http_auth=(self.username, self.password),
                    use_ssl=False,
                    verify_certs=False,
                    ssl_assert_hostname=False,
                    ssl_show_warn=False,
                    timeout=120,
                    max_retries=3
                )
                print(f"âœ… {index_type} ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def high_speed_parallel_processing(self,
                                     data_batch_size: int = 3000,
                                     indexing_batch_size: int = 100,
                                     num_workers: int = 6) -> bool:
        """ê³ ì† 3ì¸ë±ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬"""

        import gc        
        
        print(f"ğŸš€ ê³ ì† 3ì¸ë±ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
        print(f"ğŸ“Š ë°ì´í„° ë°°ì¹˜: {data_batch_size}, ì¸ë±ì‹± ë°°ì¹˜: {indexing_batch_size}")
        print(f"ğŸ‘· ì›Œì»¤ ìˆ˜: {num_workers} (ì¸ë±ìŠ¤ë‹¹ 2ê°œì”©)")
        
        # ì´ˆê¸°í™”
        if not self.connect_postgresql():
            return False
        
        total_records = self.get_address_count()
        if total_records == 0:
            return False
        
        # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        if not self.vector_stores:
            self.initialize_vector_stores()
        
        # í†µê³„ ë° í ì´ˆê¸°í™”
        self.stats['start_time'] = time.time()
        self.stats['total_processed'] = 0
        self.stats['successful'] = 0
        self.stats['failed'] = 0
        
        # ì¸ë±ìŠ¤ë³„ ì‘ì—… í (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        work_queues = {
            'road': queue.Queue(maxsize=5),
            'jibun': queue.Queue(maxsize=5),
            'building': queue.Queue(maxsize=5)
        }
        
        # ê²°ê³¼ í
        result_queue = queue.Queue()
        
        # í†µê³„ ê´€ë¦¬ìš© ë½
        stats_lock = threading.Lock()
        
        def producer_worker():
            """PostgreSQLì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ ê° ì¸ë±ìŠ¤ë³„ íì— ë¶„ë°°"""
            try:
                batch_count = 0
                for document_sets in self.stream_address_documents(data_batch_size):
                    batch_count += 1
                    print(f"ğŸ“¦ í”„ë¡œë“€ì„œ: ë°°ì¹˜ {batch_count} íì— ë¶„ë°° ì¤‘...")
                    
                    # ê° ì¸ë±ìŠ¤ë³„ë¡œ íì— ì¶”ê°€
                    for index_type, documents in document_sets.items():
                        if documents:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                            work_queues[index_type].put(documents)
                            print(f"  ğŸ“‹ {index_type}: {len(documents)}ê°œ íì— ì¶”ê°€")
                
                    # document_sets ë©”ëª¨ë¦¬ í•´ì œ
                    del document_sets
                    
                    # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
                    if batch_count % 5 == 0:
                        gc.collect()

                # ì›Œì»¤ ìˆ˜ë§Œí¼ ì¢…ë£Œ ì‹ í˜¸ ì¶”ê°€
                for index_type in work_queues:
                    for _ in range(2):  # ì¸ë±ìŠ¤ë‹¹ 2ê°œ ì›Œì»¤
                        work_queues[index_type].put(None)
                
                print(f"âœ… í”„ë¡œë“€ì„œ ì™„ë£Œ: ì´ {batch_count}ê°œ ë°°ì¹˜ ë¶„ë°°")
                
            except Exception as e:
                print(f"âŒ í”„ë¡œë“€ì„œ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒì‹œì—ë„ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡
                for index_type in work_queues:
                    for _ in range(2):
                        work_queues[index_type].put(None)
        
        def consumer_worker(index_type: str, worker_id: int):
            """ì¸ë±ìŠ¤ë³„ ì „ìš© ì›Œì»¤"""
            worker_success = 0
            worker_failed = 0
            worker_name = f"{index_type}-{worker_id}"
            
            print(f"ğŸ”§ ì›Œì»¤ {worker_name} ì‹œì‘")
            
            while True:
                try:
                    # í•´ë‹¹ ì¸ë±ìŠ¤ íì—ì„œ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
                    documents = work_queues[index_type].get(timeout=120)
                    
                    if documents is None:  # ì¢…ë£Œ ì‹ í˜¸
                        print(f"ğŸ ì›Œì»¤ {worker_name} ì¢…ë£Œ")
                        break
                    
                    print(f"  ğŸ‘· ì›Œì»¤ {worker_name}: {len(documents)}ê°œ ì²˜ë¦¬ ì‹œì‘")
                    batch_start_time = time.time()
                    
                    # ì†Œë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì¸ë±ì‹±
                    for i in range(0, len(documents), indexing_batch_size):
                        mini_batch = documents[i:i + indexing_batch_size]
                        
                        # ë¹ ë¥¸ ì¸ë±ì‹± (ì¬ì‹œë„ ìµœì†Œí™”)
                        for attempt in range(2):  # ìµœëŒ€ 2ë²ˆë§Œ ì‹œë„
                            try:
                                self.vector_stores[index_type].add_documents(mini_batch)
                                worker_success += len(mini_batch)
                                
                                # Road ì¸ë±ìŠ¤ë§Œ ì „ì²´ í†µê³„ì— ë°˜ì˜
                                if index_type == 'road':
                                    with stats_lock:
                                        self.stats['successful'] += len(mini_batch)
                                        self.stats['total_processed'] += len(mini_batch)
                                
                                break
                            except Exception as e:
                                if attempt == 1:  # ë§ˆì§€ë§‰ ì‹œë„
                                    print(f"    âŒ ì›Œì»¤ {worker_name} ì†Œë°°ì¹˜ ì‹¤íŒ¨: {str(e)[:50]}...")
                                    worker_failed += len(mini_batch)
                                    
                                    if index_type == 'road':
                                        with stats_lock:
                                            self.stats['failed'] += len(mini_batch)
                                            self.stats['total_processed'] += len(mini_batch)
                                else:
                                    time.sleep(0.3)  # ì§§ì€ ì¬ì‹œë„ ê°„ê²©
                    
                    batch_time = time.time() - batch_start_time
                    print(f"  âœ… ì›Œì»¤ {worker_name}: {len(documents)}ê°œ ì™„ë£Œ ({batch_time:.1f}ì´ˆ)")
                    
                    work_queues[index_type].task_done()
                    
                except queue.Empty:
                    print(f"â° ì›Œì»¤ {worker_name} íƒ€ì„ì•„ì›ƒ")
                    break
                except Exception as e:
                    print(f"âŒ ì›Œì»¤ {worker_name} ì˜¤ë¥˜: {e}")
            
            result_queue.put((worker_name, worker_success, worker_failed))
            return worker_success, worker_failed
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        print(f"ğŸš€ 3ì¸ë±ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì´ {num_workers + 1}ê°œ ì›Œì»¤)")
        
        # í”„ë¡œë“€ì„œ ìŠ¤ë ˆë“œ
        producer_thread = threading.Thread(target=producer_worker, daemon=True)
        producer_thread.start()
        
        # ì¸ë±ìŠ¤ë³„ ì»¨ìŠˆë¨¸ ìŠ¤ë ˆë“œë“¤
        consumer_threads = []
        
        for index_type in ['road', 'jibun', 'building']:
            for worker_id in range(1, 3):  # ì¸ë±ìŠ¤ë‹¹ 2ê°œ ì›Œì»¤
                thread = threading.Thread(
                    target=consumer_worker, 
                    args=(index_type, worker_id), 
                    daemon=True
                )
                thread.start()
                consumer_threads.append(thread)
        
        # ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§
        def monitor_progress():
            last_check = 0
            while any(t.is_alive() for t in consumer_threads):
                time.sleep(15)  # 15ì´ˆë§ˆë‹¤ ì²´í¬
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                try:
                    memory_info = psutil.virtual_memory()
                    memory_percent = memory_info.percent
                except:
                    memory_percent = 0

                with stats_lock:
                    current_success = self.stats['successful']
                    current_total = self.stats['total_processed']
                
                if current_success > last_check:
                    elapsed = time.time() - self.stats['start_time']
                    docs_per_min = current_success / (elapsed / 60) if elapsed > 0 else 0
                    progress = current_success / total_records * 100
                    
                    print(f"\nğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({current_success:,}/{total_records:,})")
                    print(f"ğŸš€ ì†ë„: {docs_per_min:.0f} docs/min")
                    print(f"ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory_percent:.1f}%")
                    print(f"ğŸ“‚ ì²˜ë¦¬ í˜„í™©: Road({current_success:,})")
                    
                    # í ìƒíƒœ í™•ì¸
                    queue_status = []
                    for idx_type, q in work_queues.items():
                        queue_status.append(f"{idx_type}({q.qsize()})")
                    print(f"ğŸ“‹ í ìƒíƒœ: {', '.join(queue_status)}")
                    
                    if docs_per_min > 0:
                        remaining_docs = total_records - current_success
                        eta_minutes = remaining_docs / docs_per_min
                        print(f"â±ï¸ ì˜ˆìƒ ì™„ë£Œ: {eta_minutes:.0f}ë¶„ í›„")
                    
                    # ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³  ë° ì •ë¦¬
                    if memory_percent > 85:
                        print(f"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ ! ì²˜ë¦¬ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                        gc.collect()
                        
                    last_check = current_success
        
        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        monitor_thread = threading.Thread(target=monitor_progress, daemon=True)
        monitor_thread.start()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        producer_thread.join()
        print("ğŸ“¦ í”„ë¡œë“€ì„œ ì™„ë£Œ")
        
        for thread in consumer_threads:
            thread.join()
        print("ğŸ‘· ëª¨ë“  ì›Œì»¤ ì™„ë£Œ")
        
        # ìµœì¢… ê²°ê³¼ ìˆ˜ì§‘
        index_results = {}
        
        while not result_queue.empty():
            try:
                worker_name, success, failed = result_queue.get_nowait()
                index_type = worker_name.split('-')[0]
                
                if index_type not in index_results:
                    index_results[index_type] = {'success': 0, 'failed': 0}
                
                index_results[index_type]['success'] += success
                index_results[index_type]['failed'] += failed
                
                print(f"ğŸ ì›Œì»¤ {worker_name} ìµœì¢… ê²°ê³¼: ì„±ê³µ {success}, ì‹¤íŒ¨ {failed}")
            except queue.Empty:
                break
        
        # PostgreSQL ì—°ê²° ì¢…ë£Œ
        if self.pg_connection:
            self.pg_connection.close()
        
        # ìµœì¢… í†µê³„
        total_time = time.time() - self.stats['start_time']
        success_rate = self.stats['successful'] / self.stats['total_processed'] * 100 if self.stats['total_processed'] > 0 else 0
        docs_per_hour = self.stats['successful'] / (total_time / 3600) if total_time > 0 else 0
        
        print(f"\nğŸ‰ ê³ ì† 3ì¸ë±ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š Road ì¸ë±ìŠ¤: {self.stats['successful']:,}ê°œ ì„±ê³µ")
        print(f"âœ… ì „ì²´ ì„±ê³µë¥ : {success_rate:.1f}%")
        print(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_time/3600:.2f}ì‹œê°„")
        print(f"ğŸš€ í‰ê·  ì†ë„: {docs_per_hour:.0f} docs/hour")
        
        # ì¸ë±ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼
        print(f"\nğŸ“‚ ì¸ë±ìŠ¤ë³„ ì²˜ë¦¬ ê²°ê³¼:")
        for index_type, results in index_results.items():
            total_docs = results['success'] + results['failed']
            success_rate_idx = results['success'] / total_docs * 100 if total_docs > 0 else 0
            print(f"  {index_type.upper():>8}: {results['success']:,}ê°œ ì„±ê³µ ({success_rate_idx:.1f}%)")
        
        return success_rate >= 70

    def analyze_query_type(self, query: str) -> str:
        """ì¿¼ë¦¬ íƒ€ì… ë¶„ì„"""
        query_lower = query.lower()
        
        # ë„ë¡œëª… íŒ¨í„´
        road_patterns = ['ë¡œ', 'ê¸¸', 'ëŒ€ë¡œ', r'\d+ë²ˆì§€', r'\d+-\d+']
        if any(pattern in query_lower or re.search(pattern, query_lower) for pattern in road_patterns):
            return 'road_focused'
        
        # ì§€ë²ˆ/ë™ëª… íŒ¨í„´
        jibun_patterns = ['ë™', 'ì', 'ë©´', 'ë¦¬', 'ë²ˆì§€']
        if any(pattern in query_lower for pattern in jibun_patterns):
            return 'jibun_focused'
        
        # ë¹Œë”©ëª… íŒ¨í„´
        building_patterns = ['ë¹Œë”©', 'íƒ€ì›Œ', 'ì„¼í„°', 'ë³‘ì›', 'í•™êµ', 'ì•„íŒŒíŠ¸', 'í˜¸í…”']
        if any(pattern in query_lower for pattern in building_patterns):
            return 'building_focused'
        
        return 'mixed'

    def smart_three_index_search(self, query: str, k: int = 5) -> List[Dict]:
        """3ì¸ë±ìŠ¤ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰"""
        
        print(f"\nğŸ” 3ì¸ë±ìŠ¤ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰: '{query}'")
        
        # ì¿¼ë¦¬ íƒ€ì… ë¶„ì„
        query_type = self.analyze_query_type(query)
        print(f"ğŸ§  ì¿¼ë¦¬ íƒ€ì…: {query_type}")
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        if query_type == 'road_focused':
            weights = {'road': 0.7, 'jibun': 0.2, 'building': 0.1}
        elif query_type == 'jibun_focused':
            weights = {'road': 0.2, 'jibun': 0.7, 'building': 0.1}
        elif query_type == 'building_focused':
            weights = {'road': 0.1, 'jibun': 0.1, 'building': 0.8}
        else:  # mixed
            weights = {'road': 0.4, 'jibun': 0.3, 'building': 0.3}
        
        print(f"âš–ï¸ ê°€ì¤‘ì¹˜: {weights}")
        
        all_results = []
        
        # ê° ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰
        for index_type, weight in weights.items():
            if weight <= 0:
                continue
                
            try:
                results = self.vector_stores[index_type].similarity_search_with_score(
                    query, k=k*2  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ ë‹¤ì–‘ì„± í™•ë³´
                )
                
                print(f"  ğŸ“‹ {index_type}: {len(results)}ê°œ ê²°ê³¼")
                
                for doc, score in results:
                    # ê°€ì¤‘ì¹˜ ì ìš©í•œ ì ìˆ˜ ê³„ì‚°
                    weighted_score = score * weight
                    
                    result = {
                        'document': doc,
                        'original_score': score,
                        'weighted_score': weighted_score,
                        'source_index': index_type,
                        'weight': weight
                    }
                    all_results.append(result)
                    
            except Exception as e:
                print(f"  âŒ {index_type} ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        unique_results = self.deduplicate_results(all_results)
        
        # ê°€ì¤‘ì¹˜ ì ìˆ˜ë¡œ ì •ë ¬
        unique_results.sort(key=lambda x: x['weighted_score'], reverse=True)
        
        # ìƒìœ„ kê°œ ë°˜í™˜
        final_results = unique_results[:k]
        
        print(f"  ğŸ¯ ìµœì¢… ê²°ê³¼: {len(final_results)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
        
        return final_results

    def deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ ì¤‘ë³µ ì œê±° (ë„ë¡œëª…ì£¼ì†Œ ê¸°ì¤€)"""
        seen_addresses = {}
        
        for result in results:
            road_addr = result['document'].metadata.get('road_address', '')
            
            if road_addr not in seen_addresses:
                seen_addresses[road_addr] = result
            else:
                # ê¸°ì¡´ ê²°ê³¼ë³´ë‹¤ ì ìˆ˜ê°€ ë†’ìœ¼ë©´ êµì²´
                if result['weighted_score'] > seen_addresses[road_addr]['weighted_score']:
                    seen_addresses[road_addr] = result
        
        return list(seen_addresses.values())

    def search_test(self, test_queries: List[str], k: int = 5):
        """3ì¸ë±ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ” 3ì¸ë±ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (Road + Jibun + Building)")
        print("=" * 70)
        
        if not self.vector_stores:
            self.initialize_vector_stores()
        
        for query in test_queries:
            print(f"\nğŸ” ê²€ìƒ‰ì–´: '{query}'")
            print("-" * 50)
            
            try:
                results = self.smart_three_index_search(query, k=k)
                
                if not results:
                    print("  âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    continue
                
                print(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                
                for i, result in enumerate(results):
                    doc = result['document']
                    road_addr = doc.metadata.get('road_address', '')
                    jibun_addr = doc.metadata.get('jibun_address', '')
                    building_name = doc.metadata.get('building_name', '')
                    source_index = result['source_index']
                    weighted_score = result['weighted_score']
                    original_score = result['original_score']
                    
                    print(f"\n  ğŸ“ {i+1}ë²ˆì§¸ ê²°ê³¼ [ì¶œì²˜: {source_index.upper()}]")
                    print(f"     ğŸ  ë„ë¡œëª…: {road_addr}")
                    if jibun_addr and jibun_addr != road_addr:
                        print(f"     ğŸ“® ì§€ë²ˆ: {jibun_addr}")
                    if building_name:
                        print(f"     ğŸ¢ ê±´ë¬¼ëª…: {building_name}")
                    
                    print(f"     ğŸ¯ ê°€ì¤‘ì¹˜ ì ìˆ˜: {weighted_score:.4f} (ì›ë³¸: {original_score:.4f})")
                    print(f"     ğŸ“„ ê²€ìƒ‰ëœ ë‚´ìš©: {doc.page_content[:80]}...")
                        
            except Exception as e:
                print(f"  âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    def get_index_statistics(self):
        """ì¸ë±ìŠ¤ë³„ í†µê³„ ì¡°íšŒ"""
        print(f"\nğŸ“Š 3ì¸ë±ìŠ¤ í†µê³„")
        print("="*50)
        
        for index_type, index_name in self.indexes.items():
            try:
                response = requests.get(
                    f"{self.opensearch_url}/{index_name}/_count",
                    auth=self.auth,
                    verify=self.verify_ssl
                )
                
                if response.status_code == 200:
                    count = response.json().get('count', 0)
                    print(f"ğŸ“‚ {index_type.upper():>8}: {count:,}ê°œ ë¬¸ì„œ")
                else:
                    print(f"âŒ {index_type.upper():>8}: ì¡°íšŒ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"âŒ {index_type.upper():>8}: ì˜¤ë¥˜ - {e}")

    def benchmark_search_methods(self, test_queries: List[str], k: int = 3):
        """ê²€ìƒ‰ ë°©ë²•ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸƒâ€â™‚ï¸ 3ì¸ë±ìŠ¤ ê²€ìƒ‰ ë°©ë²•ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("=" * 70)
        
        methods = [
            ('Road ë‹¨ë…', lambda q: self.vector_stores['road'].similarity_search(q, k=k)),
            ('Jibun ë‹¨ë…', lambda q: self.vector_stores['jibun'].similarity_search(q, k=k)),
            ('Building ë‹¨ë…', lambda q: self.vector_stores['building'].similarity_search(q, k=k)),
            ('3ì¸ë±ìŠ¤ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰', lambda q: [r['document'] for r in self.smart_three_index_search(q, k=k)])
        ]
        
        for query in test_queries:
            print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'")
            print("-" * 40)
            
            for method_name, search_func in methods:
                try:
                    start_time = time.time()
                    results = search_func(query)
                    search_time = time.time() - start_time
                    
                    print(f"  ğŸ“‹ {method_name:15}")
                    print(f"    â±ï¸ ê²€ìƒ‰ ì‹œê°„: {search_time*1000:.1f}ms")
                    print(f"    ğŸ“Š ê²°ê³¼ ìˆ˜: {len(results)}ê°œ")
                    
                    if results:
                        if hasattr(results[0], 'metadata'):
                            first_result = results[0].metadata.get('road_address', '')[:50]
                        else:
                            first_result = str(results[0])[:50]
                        print(f"    ğŸ¥‡ 1ìˆœìœ„: {first_result}...")
                    
                except Exception as e:
                    print(f"  âŒ {method_name}: ì˜¤ë¥˜ - {str(e)[:30]}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ‡°ğŸ‡· ìµœì¢… 3ì¸ë±ìŠ¤ ê³ ì† ë³‘ë ¬ RAG ì‹œìŠ¤í…œ")
    print("=" * 70)
    print("ğŸ”§ êµ¬ì¡°:")
    print("  â€¢ Road ì¸ë±ìŠ¤: ë„ë¡œëª…ì£¼ì†Œ ì „ìš© (ë¹Œë”©ëª… ì œì™¸)")
    print("  â€¢ Jibun ì¸ë±ìŠ¤: ì§€ë²ˆì£¼ì†Œ ì „ìš©")
    print("  â€¢ Building ì¸ë±ìŠ¤: ë¹Œë”©ëª…+ì§€ì—­ (aliases ì œê±°)")
    print("=" * 70)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = FinalThreeIndexRAG()
    
    # OpenSearch ì—°ê²° í™•ì¸
    if not rag_system.check_opensearch_connection():
        print("âŒ OpenSearch ì—°ê²° ì‹¤íŒ¨")
        return
    
    print("\nğŸ“‹ ì‹¤í–‰ ì˜µì…˜:")
    print("1. ğŸ”¥ ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ (3ì¸ë±ìŠ¤ ìƒì„± + ë³‘ë ¬ ì²˜ë¦¬)")
    print("2. ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚¬ìš©)")
    print("3. ğŸ“Š ì¸ë±ìŠ¤ í†µê³„ í™•ì¸")
    print("4. ğŸƒâ€â™‚ï¸ ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("5. ğŸ§¹ ëª¨ë“  ì¸ë±ìŠ¤ ì‚­ì œ")
    print("6. ì¢…ë£Œ")
    
    choice = input("\nì„ íƒ (1-6): ").strip()
    
    if choice == "1":
        # ê³ ì† ë³‘ë ¬ ì²˜ë¦¬
        if not rag_system.create_three_indexes():
            print("âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            return
        
        print("\nâš™ï¸ ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •:")
        workers = int(input("ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ 6, ê¶Œì¥ 4-8): ") or "6")
        data_batch = int(input("ë°ì´í„° ë°°ì¹˜ (ê¸°ë³¸ 3000): ") or "3000")
        index_batch = int(input("ì¸ë±ì‹± ë°°ì¹˜ (ê¸°ë³¸ 100): ") or "100")
        
        print(f"\nğŸš€ ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘!")
        print(f"ğŸ“Š ì„¤ì •: ì›Œì»¤ {workers}ê°œ, ë°ì´í„°ë°°ì¹˜ {data_batch}, ì¸ë±ì‹±ë°°ì¹˜ {index_batch}")
        print(f"âš¡ ì˜ˆìƒ ì²˜ë¦¬ì†ë„: 15,000-30,000 docs/hour")
        
        success = rag_system.high_speed_parallel_processing(
            data_batch_size=data_batch,
            indexing_batch_size=index_batch,
            num_workers=workers
        )
        
        if success:
            print("\nâœ… ê³ ì† ì²˜ë¦¬ ì„±ê³µ!")
            rag_system.get_index_statistics()
            
            # ìë™ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\nğŸ” ìë™ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            test_queries = [
                "í…Œí—¤ë€ë¡œ2ê¸¸ 21",          # Road ì¤‘ì‹¬
                "ì—­ì‚¼ë™ 825-27",           # Jibun ì¤‘ì‹¬
                "ì„¸ë¸Œë€ìŠ¤ë³‘ì›",            # Building ì¤‘ì‹¬
                "ì„œìš¸ ì‚¼ì„±ë¹Œë”©",           # Building + ì§€ì—­
                "ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ"          # Road + ì§€ì—­
            ]
            
            rag_system.search_test(test_queries, k=3)
            
        else:
            print("âŒ ê³ ì† ì²˜ë¦¬ ì‹¤íŒ¨")
    
    elif choice == "2":
        print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        
        test_queries = []
        print("\nê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°ë§Œ ì¹˜ë©´ ê¸°ë³¸ ê²€ìƒ‰ì–´ ì‚¬ìš©):")
        
        while True:
            query = input("ê²€ìƒ‰ì–´: ").strip()
            if not query:
                break
            test_queries.append(query)
        
        if not test_queries:
            test_queries = [
                "í…Œí—¤ë€ë¡œ2ê¸¸ 21",          # ë„ë¡œëª…ì£¼ì†Œ
                "ì—­ì‚¼ë™ 825",              # ì§€ë²ˆì£¼ì†Œ
                "ì„¸ë¸Œë€ìŠ¤ë³‘ì›",            # ë¹Œë”©ëª…
                "ì„œìš¸ ì‚¼ì„±ë¹Œë”©",           # ì§€ì—­+ë¹Œë”©
                "ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ",         # ì§€ì—­+ë„ë¡œëª…
                "ë¶€ì‚° ë¡¯ë°í˜¸í…”",           # ì§€ì—­+ë¹Œë”©
                "ì—°ì„¸ë¡œ 50-1",             # ë„ë¡œëª…+ë²ˆì§€
                "ì‹ ì´Œë™"                   # ë™ëª…
            ]
            print("ê¸°ë³¸ ê²€ìƒ‰ì–´ ì‚¬ìš©")
        
        rag_system.search_test(test_queries, k=5)
    
    elif choice == "3":
        rag_system.get_index_statistics()
    
    elif choice == "4":
        print("\nğŸƒâ€â™‚ï¸ ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        
        benchmark_queries = [
            "í…Œí—¤ë€ë¡œ2ê¸¸ 21",          # ë„ë¡œëª… ì •í™• ê²€ìƒ‰
            "ì„¸ë¸Œë€ìŠ¤ë³‘ì›",            # ë¹Œë”©ëª… ê²€ìƒ‰
            "ì—­ì‚¼ë™",                 # ë™ëª… ê²€ìƒ‰
            "ì„œìš¸ ì‚¼ì„±",              # ì§€ì—­+ë¹Œë”© ë¶€ë¶„ê²€ìƒ‰
            "ê°•ë‚¨êµ¬ ë³‘ì›",            # ì§€ì—­+ì¹´í…Œê³ ë¦¬
        ]
        
        rag_system.benchmark_search_methods(benchmark_queries)

    elif choice == "5":
        confirm = input("âš ï¸ ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if confirm == 'y':
            for index_type, index_name in rag_system.indexes.items():
                rag_system._delete_index_if_exists(index_name)
            print("âœ… ëª¨ë“  ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ")
        else:
            print("âŒ ì‚­ì œ ì·¨ì†Œ")
    
    elif choice == "6":
        print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()